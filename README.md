# Transformer Encoder-Decoder (PyTorch Educational)

Educational Transformer implementation built from scratch in PyTorch.

## Features

- Pre-norm encoder-decoder architecture
- Multi-head attention with sinusoidal positional embeddings
- Training loop and autoregressive generation example

Built as a learning resource to understand Transformer internals without abstractions (other than PyTorch).

_I know talk is cheap in the age of AI, and it's not really possible to prove to you that I wrote this without AI, but I did_

